

### Question 1 - MapReduce and Spark (28 Marks)

a) (12 Marks)Briefly describe how is MapReduce Shuffle works in Map task and reduce task.
Map:
1. each map task will produce some data, and those data will be transported to an annular buffer. 
2. Based on the key and the rule of partioner, the partitioner divide the result of the map task into different partition.
3. In this annular buffer, the result will be sorted by the key.
4. The system will merge files in the same partition
5. When the size of data in such buffer reach a threshold, the data will be spilled to disks.
Reduceï¼š
1. According to the instructions of Job Tracker, the reduce task will request the data produced by the map task. 

b) (8 Marks)Think about the procedure of Mapreduce. If we have large data, what are problems we may encounter in the MapReduce Shuffle?
1. Mermory consuming will be a bottleneck of MapReduce. When we need to process a lot of data, map tasks will require us to create more annular buffers.
2. Network IO will be a problem. In the scenario of large amount of data, we must store data in different machine. Therefore, the request of reduce tasks will increase the pressure of network IO.
3. The speed of disk I/O. The more data we have to process, the more time will be wasted due to the speed of disk IO.
4. Data may be unevenly distributed. If you select a bad partion schema as your partitionor, some partitions are very large but some partions are very small.

c) (8 Marks)Dr. X argues that the bottleneck of mapreduce is disk I/O speed, so the reason why Apache Spark is faster than MapReduce in shuffle merely is that Apache Spark writes data to the memory rather than to the disk. Do you agree with Dr. X's statement? Why/ Why not?
No, this statement is partially right. The bottleneck of mapreduce is network I/O and memory shortage, because of the heavy traffic and buffers to store intermediate data. 
1. In MapReduce, map tasks will generate many buckets with small size, and this will make shuffle  more frequently. Howerver, Spark will merge those buckets into a single file and shuffle will be less, which means the job will be less time-consuming. 
2. Map tasks output the calculate result one by one, and use AppendOnlyMap and its aggregate algorithm to aggregate middle results, which greatly reduce the memory middle results used. Therefore, Spark will have more memory to do other mapreduce tasks. 
3. Reduce tasks will read middle results of map tasks one by one, and sort and aggreate those data in memory. Obiviously, Spark needn't consume memory to store unprocessed data. Therefore, Spark have more mermory to process other data and Spark becomes faster. 
4. Reduce tasks will pull the blocks according to the distribution of BlockManager Address, and blocks in the same BlockManger address will be accumulated. Spark will request those blocks in a batch, which will significately reduce the network I/O pressure. Therefore, Spark will have more network IO to do other mapreduce tasks. 

### Question  - Kafka and Storage System(20 marks)

You are working in a software company. The company is interested in distributed storage middlewares. According to the requirements of the market, the company decided to build a distributed storage system with approximately equal frequency of reading and writing. Luckily, the technical team of your company find Kafka may be similar to this product. Your supervisor has assigned you to study how to achieve this task by the experience of Apache Kafka. 

[To explain your answer, it would be instructive to discuss this with Network/File IO, Disk IO, read/write of partition, High Availablity]

1. In terms of network/file IO, Kafka has adopted the techniquae of zero-copy and page cache and this will make the I/O much faster. Traditionally, if we want to read the data from file/network and write it into file/network, we have to pay much time on switching userspace to kernel space. This is mainly because we have to read the data from NIC buffer/disk buffer in kernel space to user space and load the data to kernel space in order to write it to DISK buffer/NIC buffer. However, Kafka use `sendfile` to directly immigrate the data from NIC buffer to disk buffer.
2. In terms of disk IO, sequence read and write plays a significant role in the high efficiency of Kafka. This is mainly because that disk is more friendly to sequence read and write. If the disk has to read/write some random data, the head has to continuously adjust its position and this will make read/write more time-consuming. In this case, Kafka taught us to try to accumulate your information in a batch and sequentially arrange it. 
3. With the approximately equal frequency of reading and write, the Kafka team decided to use the leader to read and write, rather than the leader write and followers read. In this circumstance, the leader has to deal with so many write requests. If the team of Kafka uses the architecture of the leader to write and followers read, due to the latency of sync, the user must read some dirty data and the reliability will decrease. 
4. To achieve the goal of high availability, Kafka has introduced to the concept of replica and ISR. The ISR represents the replica whose message is similar to the leader's, and when the leader is dead, Kafka will select one of those as the leader. So, if we want to make our system to reach the goal of high availability, we don't need every replica to have to be similar to the leader, and we just need some.

### Question 2

When doing streaming works, what is the difference between Storm and Spark Streaming?



### Question 3

Why Apache Storm cannot realize the exactly-once semantics? (Please think about the implement of exactly-once semantics in Kafka.) If you are an Apache Storm Committer, how will you achieve this goal?

The main reason of this issue is that Apache Storm has no state management. To be more specific, Apache Storm cannot perceive whether the system have processed the data or not. Also, Apache Storm cannot implement commit and rollback. 

Therefore, I think we should adopt two-phase commit(aka. 2pc). Like Kafka, we can use an object called Transaction Coordinator. Besides, every spout/bolt needs to report its state to the state manager and it will store those states to storage. When there are some errors occurred in a bolt/spout, the System will read the state from the storage and recover it immediately. Finally, when every bolt/spout in the system has successfully finished its pre-commit. The Transaction Coordinator will inform every spout/bolt that the transaction have finished. 


### Question 5

1. How we deal with big data? What are relationships of HDFS, MapReduce and HBase?

With data becomes larger and larger, we need to arrange it, process it and store it. Therefore, we use HBase to arrange those data, MapReduce to  process it and HDFS to store it.

HDFS v.s. BigTable

BigTable v.s. Dynamon
