

### Question 1 - MapReduce and Spark (28 Marks)

a) (12 Marks)Briefly describe how is MapReduce Shuffle works in Map task and reduce task.
1. each map task will produce some data, and those data will be transported to an annular buffer. 
2. Based on the key and the rule of partioner, the partitioner divide the result of the map task into different partition.
3. In this annular buffer, the result will be sorted by the key.
4. The system will merge files in the same partition
5. When the size of data in such buffer reach a threshold, the data will be spilled to disks.
6. According to the instructions of Job Tracker, the reduce task will request the data produced by the map task. 

b) (8 Marks)Think about the procedure of Mapreduce. If we have large data, what are problems we may encounter in the MapReduce Shuffle?
1. Mermory consuming will be a bottleneck of MapReduce. When we need to process a lot of data, map tasks will require us to create more annular buffers.
2. Network IO will be a problem. In the scenario of large amount of data, we must store data in different machine. Therefore, the request of reduce tasks will increase the pressure of network IO.
3. The speed of disk I/O. The more data we have to process, the more time will be wasted due to the speed of disk IO.
4. Data may be unevenly distributed. If you select a bad partion schema as your partitionor, some partitions are very large but some partions are very small.

c) (8 Marks)Dr. X argues that the bottleneck of mapreduce is disk I/O speed, so the reason why Apache Spark is faster than MapReduce in shuffle merely is that Apache Spark writes data to the memory rather than to the disk. Do you agree with Dr. X's statement? Why/ Why not?
No, this statement is partially right. The bottleneck of mapreduce is network I/O and memory shortage, because of the heavy traffic and buffers to store intermediate data. 
1. In MapReduce, map tasks will generate many buckets with small size, and this will make shuffle  more frequently. Howerver, Spark will merge those buckets into a single file and shuffle will be less, which means the job will be less time-consuming. 
2. Map tasks output the calculate result one by one, and use AppendOnlyMap and its aggregate algorithm to aggregate middle results, which greatly reduce the memory middle results used. Therefore, Spark will have more memory to do other mapreduce tasks. 
3. Reduce tasks will read middle results of map tasks one by one, and sort and aggreate those data in memory. Obiviously, Spark needn't consume memory to store unprocessed data. Therefore, Spark have more mermory to process other data and Spark becomes faster. 
4. Reduce tasks will pull the blocks according to the distribution of BlockManager Address, and blocks in the same BlockManger address will be accumulated. Spark will request those blocks in a batch, which will significately reduce the network I/O pressure. Therefore, Spark will have more network IO to do other mapreduce tasks. 

### Question 2 - Kafka and Storage System(20 marks)

You are working in a software company. The company is interested in distributed storage middlewares. According to the requirements of the market, the company decided to build a distributed storage system with approximately equal frequency of reading and writing. Luckily, the technical team of your company find Kafka may be similar to this product. Your supervisor has assigned you to study how to achieve this task by the experience of Apache Kafka. 

a) (8 Marks)The IO is the most important infrastructure for a storage system. What solution you can come up with in this part?
1. In terms of network/file IO, Kafka has adopted the technique of zero-copy and page cache and this will make the I/O much faster. Traditionally, if we want to read the data from file/network and write it into file/network, we have to pay much time on switching userspace to kernel space. This is mainly because we have to read the data from NIC buffer/disk buffer in kernel space to user space and load the data to kernel space in order to write it to DISK buffer/NIC buffer. However, Kafka use `sendfile` to directly immigrate the data from NIC buffer to disk buffer.
2. In terms of disk IO, sequence read and write plays a significant role in the high efficiency of Kafka. This is mainly because that disk is more friendly to sequence read and write. If the disk has to read/write some random data, the head has to continuously adjust its position and this will make read/write more time-consuming. In this case, Kafka taught us to try to accumulate your information in a batch and sequentially arrange it.

b) (4 Marks)In old version of Apache Kafka, the team use zookeeper extensively. However, some developer in the community had pointed out that there are some problems in this model. What are those problems? 
1. Herd effect: If a node which are listened by many client, many wacher notification will send to clients.
2. Brain split: In the same moment, the consumer may receive inconsistent status.

c) (4 Marks)the Read/Write Model is significantly important for a Storage System. What is the model of Kafka have adopted and why?
With the approximately equal frequency of reading and write, the Kafka team decided to use the leader to read and write, rather than the leader write and followers read. In this circumstance, the leader has to deal with so many write requests. If the team of Kafka uses the architecture of the leader to write and followers read, due to the latency of sync, the user must read some dirty data and the reliability will decrease. 

d) (4 Marks) High availablity plays a significant role in a distributed system. How Kafak achieve this? 
To achieve the goal of high availability, Kafka has introduced to the concept of replica and ISR. The ISR represents the replica whose message is similar to the leader's, and when the leader is dead, Kafka will select one of those as the leader. So, if we want to make our system to reach the goal of high availability, we don't need every replica to have to be similar to the leader, and we just need some.

### Question 3 - Deployment mode(15 Marks)

Generally, every big data computation framework will have three kinds of deployment modes: YARN, K8S and standalone. 
1. (5 Marks)In MapReduce 1.0, the Hadoop team didn't come up with YARN. However, in MapReduce 2.0, they introduce YARN to us. 
 i) (1 Marks)What is YARN used for?
    YARN is used for resources management
 ii) (1 Marks)Why they do this? 
    The JobTrack in MapReduce 1.0 have to schedule resources and schedule jobs. This will make JobTracker very busy.
 iii) (3 Marks) How do YARN works?
    Firstly, every applications wants to run on YARN must apply resources to run the application's Application Master. Then, YARN will assign some resources to run container which will be used to run Slaves. Therefore, the application only needs to focus on their tasks and needn't worry about the resource allocation.

2. (5 Marks)


3. (5 Marks)



### Question 4 - Stream Computation(19 Marks )

Nowadays, Streaming computation becomes more and more popular. 
a. (3 Marks)Doctor Y argues that in a ideal environment, streaming computation is able to convert to batch computation. Do you agree with Dr. Y's statement? Why/ Why not?
Yes, I agree with Doctor Y's statement. In the case, we only need to append in-boundary and out-boundary in the stream and the data between the out-boundary and the in-boundary is batched. this is what so-called convert stream computation to batch computation.

b. (6 Marks)Name three different Message delivery rules in Apache Storm. If it can realize on Apache Storm, describe how it works. If not, explain why. 
1. at-most-once semantics: the upstream bolt/spout send the message only once, and it dosen't care about whether downstream have received the message. 
2. at-least-once semantics: the upstream bolt/spout will send the message at least once, and every downstream will reply a ack to it when the downstream bolt have received the message. If the upstream bolt/spout haven't received all acks, it will resend the message.
3. exactly-once semantics:
Apache Storm cannot realize this feature. This is mainly because Apache Storm has no state persistence. Obiviously, if the message fail to deliver to part of downstream bolt, the bolt have received this message cannot rollback to the former state. Therefore, Apache Storm cannot achieve this goal.

c. (6 Marks) If exactly-once semantic can be implement in Apache Storm, please describe how it works. if not, imagine you are an Apache Storm Committer, how will you achieve this goal?
I will acheive this in two parts.
1. I will introduce state persistence into Apache Storm. This means will need to store the state into Memory or Disk(DB, FileSystem). After a period of time, the System need to get checkpoints of bolts/spouts. 
2. I will use two-phase commit(aka. 2pc) to do transaction. Let's say, use an object called Transaction Coordinator to coordinator the transaction. Firstly, the System will do some pre-commit. If there are some errors occurred in a bolt/spout, it will report the error to the coordinator and the coordinator will inform downstream bolts to rollback. If the coordinator dosen't receive any error report, the coordinator will inform every spout/bolt that the transaction have finished, which means we have finished the exactly-once semantics delivert.

d. (4 Marks) Storm guarantees that every message sent by the sput will be fully processed. Do you know how it works? You need to draw a graph and explain this. 


### Question 5 - NoSQL Database (18Marks)

1. i) (2 Marks)What is the procedure of read operation and write operation in HBase? Please briefly describe it. 
Write: 
when HBase receive a write request from a client, it will append it into memtable. When memtable reach a threshold, it will write data in to SSTable.
Read:
Firstly, HBase will search this in the memtale. If HBase cannot find it in memtable, HBase will search it in the SSTable. 
ii) (4 Marks)How does Bloom Filter works. Besides, can you tell other applications of Bloom Filter?
Firstly, we need to prepare a large bit map and prepare a set of hash functions. Then, we need to respectively hash the key according to those hash function. For each function, if we get n by the function of f, we set n to 1. Then, when we want to find x whether exist in the SSTable, we need to hash x by those hash function. let's say, we get h1, h2, h3, h4, .. hn. Finally, we need to see the result of h1&h2&h3&h4...&hn. If the answer is 1, this represents x exist in this SSTable, otherwise. 
One application of Bloom Filter is in Redis. By using Bloom Filter, Redis can quickly find out whether a key exist in the database. 

2. (6 Marks) Based on the read operation and write operation in HBase, what kind of scenario is HBase suitable for and why? Besides, What is the design philosophy behind it for us to learn and try to explian this philosophy?
Due to the sequential write and compact write operation, the write performance of HBase will be excellent. Therefore, HBase is suitable for write more. 
The design philosophy behind HBase is transform random writes to sequential write and bring many duplicate write. Generally, the data of RDBMS will be appended row by row, which need to random access the disk. This is called random writes. However, HBase will append data batch by batch, which means sequential access the disk. In order to get a sorted list, HBase will continuously merge SSTables and generate a sorted SSTable. This process is called major compaction. Apprently, this will cause some duplicated writting.  

3. Dynamo is famous for its High availablity. Please think about following questions:
i) (3 Marks ) What is the obstacle of the high availabilty for writers and how Dynamo acheive this?
The main problem of the high availabilty for writers is that the data will have different version and a key will have different values. Dynamo acheive this by Vector clocks. 

ii) (3 Marks) Please explain how Dynamo will do for tempoary failure and permanent failure? And How Dynamon detect the inconsistencies?
For tempoary failure, Dynamo use the technique of sloppy qourum and hinted handoff. Replica will be stored on healthy nodes downstream the ring. Then, when the node recover, those nodes will send data to this node.
For permanent failure, Dynamo will send replica to other nodes. Also, Dynamo use Merkle Tree to detect the inconsistcies between data stored by replicas.
